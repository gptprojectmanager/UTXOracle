#!/usr/bin/env python3
"""
Import Historical HTML Data to DuckDB

Parses 672 days of historical UTXOracle HTML files and imports data into DuckDB.
Extracts: date, price from const prices = [...] JavaScript array

Usage:
    python scripts/import_historical_data.py
    python scripts/import_historical_data.py --dry-run  # Preview only
"""

import os
import sys
import re
import argparse
import logging
from pathlib import Path
from typing import Optional, Dict

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import duckdb
from dotenv import load_dotenv

# Load config
load_dotenv()
DUCKDB_PATH = os.getenv(
    "DUCKDB_PATH", "/media/sam/2TB-NVMe/prod/apps/utxoracle/data/utxoracle_cache.db"
)
HISTORICAL_DIR = Path(__file__).parent.parent / "historical_data" / "html_files"

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s"
)


def parse_html_file(file_path: Path) -> Optional[Dict]:
    """
    Parse UTXOracle HTML file and extract price data.

    Args:
        file_path: Path to HTML file

    Returns:
        dict with keys: date, price, confidence, tx_count
        None if parsing fails
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        # Extract date from filename: UTXOracle_YYYY-MM-DD.html
        filename = file_path.name
        match = re.search(r"UTXOracle_(\d{4}-\d{2}-\d{2})\.html", filename)
        if not match:
            logging.warning(f"Could not extract date from filename: {filename}")
            return None

        date_str = match.group(1)

        # Extract price array: const prices = [value1, value2, ...]
        # Pattern matches: const prices = [107436.54..., ...]
        prices_match = re.search(r"const prices = \[([\d., ]+)\]", content)
        if not prices_match:
            logging.warning(f"No prices array found in {filename}")
            return None

        # Parse array and take last value (final price of day)
        prices_str = prices_match.group(1)
        # Split by comma, clean whitespace, convert to float
        prices_list = []
        for p in prices_str.split(","):
            p_clean = p.strip()
            if p_clean:
                try:
                    prices_list.append(float(p_clean))
                except ValueError:
                    continue

        if not prices_list:
            logging.warning(f"Empty prices array in {filename}")
            return None

        # Use average as daily price (more stable than last value)
        price = sum(prices_list) / len(prices_list)

        # Try to extract confidence (may not exist in all files)
        confidence = 1.0  # Default high confidence for historical data
        conf_match = re.search(r'confidence["\']?\s*:\s*([0-9.]+)', content)
        if conf_match:
            confidence = float(conf_match.group(1))

        # Try to extract tx_count
        tx_count = len(prices_list)  # Use number of price points as proxy
        tx_match = re.search(r'tx_count["\']?\s*:\s*([0-9]+)', content)
        if tx_match:
            tx_count = int(tx_match.group(1))

        return {
            "date": date_str,
            "price": round(price, 2),
            "confidence": confidence,
            "tx_count": tx_count,
        }

    except Exception as e:
        logging.error(f"Error parsing {file_path}: {e}")
        return None


def import_to_duckdb(data_records: list, db_path: str, dry_run: bool = False):
    """
    Import parsed data into DuckDB.

    Args:
        data_records: List of dicts with parsed data
        db_path: Path to DuckDB file
        dry_run: If True, only preview without inserting
    """
    if dry_run:
        logging.info("[DRY-RUN] Would insert into DuckDB:")
        for record in data_records[:5]:  # Show first 5
            logging.info(f"  {record}")
        logging.info(f"  ... and {len(data_records) - 5} more records")
        return

    try:
        with duckdb.connect(db_path) as conn:
            # Ensure table exists
            conn.execute("""
                CREATE TABLE IF NOT EXISTS prices (
                    timestamp TIMESTAMP PRIMARY KEY,
                    utxoracle_price DECIMAL(12, 2),
                    mempool_price DECIMAL(12, 2),
                    confidence DECIMAL(5, 4),
                    tx_count INTEGER,
                    diff_amount DECIMAL(12, 2),
                    diff_percent DECIMAL(6, 2),
                    is_valid BOOLEAN DEFAULT TRUE
                )
            """)

            # Insert records
            insert_sql = """
                INSERT OR REPLACE INTO prices (
                    timestamp, utxoracle_price, mempool_price, confidence,
                    tx_count, diff_amount, diff_percent, is_valid
                ) VALUES (?, ?, NULL, ?, ?, NULL, NULL, ?)
            """

            inserted = 0
            for record in data_records:
                try:
                    # Convert date to timestamp (assume noon UTC)
                    timestamp = f"{record['date']} 12:00:00"

                    # Validate confidence (0-1 range)
                    confidence = max(0.0, min(1.0, record["confidence"]))

                    # Mark as valid if confidence >= 0.3
                    is_valid = confidence >= 0.3

                    conn.execute(
                        insert_sql,
                        [
                            timestamp,
                            record["price"],
                            confidence,
                            record["tx_count"],
                            is_valid,
                        ],
                    )
                    inserted += 1

                except Exception as e:
                    logging.warning(f"Failed to insert {record['date']}: {e}")
                    continue

            logging.info(
                f"Successfully inserted {inserted}/{len(data_records)} records"
            )

    except Exception as e:
        logging.error(f"Database error: {e}")
        raise


def main():
    parser = argparse.ArgumentParser(
        description="Import historical HTML data to DuckDB"
    )
    parser.add_argument(
        "--dry-run", action="store_true", help="Preview only, don't insert"
    )
    parser.add_argument("--limit", type=int, help="Limit number of files to process")
    args = parser.parse_args()

    # Check historical directory exists
    if not HISTORICAL_DIR.exists():
        logging.error(f"Historical directory not found: {HISTORICAL_DIR}")
        sys.exit(1)

    # Get all HTML files
    html_files = sorted(HISTORICAL_DIR.glob("UTXOracle_*.html"))
    if not html_files:
        logging.error(f"No HTML files found in {HISTORICAL_DIR}")
        sys.exit(1)

    logging.info(f"Found {len(html_files)} HTML files")

    # Apply limit if specified
    if args.limit:
        html_files = html_files[: args.limit]
        logging.info(f"Limited to first {args.limit} files")

    # Parse all files
    data_records = []
    for i, file_path in enumerate(html_files):
        if i % 50 == 0:
            logging.info(f"Processing {i}/{len(html_files)}...")

        record = parse_html_file(file_path)
        if record:
            data_records.append(record)

    logging.info(f"Successfully parsed {len(data_records)}/{len(html_files)} files")

    if not data_records:
        logging.error("No valid data extracted")
        sys.exit(1)

    # Import to DuckDB
    import_to_duckdb(data_records, DUCKDB_PATH, dry_run=args.dry_run)

    # Summary stats
    if data_records:
        prices = [r["price"] for r in data_records]
        logging.info(f"Price range: ${min(prices):,.2f} - ${max(prices):,.2f}")
        logging.info(
            f"Date range: {data_records[0]['date']} - {data_records[-1]['date']}"
        )


if __name__ == "__main__":
    main()
