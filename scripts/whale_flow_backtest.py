"""
Whale Flow Backtest - Historical Validation

Analyzes historical blocks to validate whale flow correlation with price movements.

Usage:
    python3 whale_flow_backtest.py --days 1 --csv data/exchange_addresses.csv
    python3 whale_flow_backtest.py --start-block 920000 --end-block 920144

Success Criteria (from spec.md):
- SC-002: Correlation >0.6 on 7-day backtest
- SC-003: False positive rate <20%

Performance Warning:
- Sequential processing: ~3 minutes per block (electrs HTTP API)
- 1 day (144 blocks) = ~7 hours
- 7 days (1,008 blocks) = ~50 hours
- Consider running overnight or with reduced sample size

References:
- specs/004-whale-flow-detection/spec.md (SC-002, SC-003)
- scripts/whale_flow_detector.py (WhaleFlowDetector class)
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import List, Tuple
import json
from datetime import datetime

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))
sys.path.insert(0, str(Path(__file__).parent))

# Import whale detector
from whale_flow_detector import WhaleFlowDetector

# Import contract
specs_contracts_path = (
    Path(__file__).parent.parent / "specs" / "004-whale-flow-detection" / "contracts"
)
if str(specs_contracts_path) not in sys.path:
    sys.path.insert(0, str(specs_contracts_path))

from whale_flow_detector_interface import WhaleFlowSignal


# Configuration
ELECTRS_API_URL = "http://localhost:3001"
BLOCKS_PER_DAY = 144  # Bitcoin: ~10 minutes per block = 144 blocks/day

logger = logging.getLogger(__name__)


def calculate_block_range(
    days: int = None, start_block: int = None, end_block: int = None
) -> Tuple[int, int]:
    """
    T068-T069: Calculate block range for backtest.

    Args:
        days: Number of days to backtest (if provided, uses latest blocks)
        start_block: Custom start block height
        end_block: Custom end block height

    Returns:
        Tuple of (start_block, end_block)

    Raises:
        ValueError: If arguments are invalid
    """
    import requests

    # Validate arguments
    if days and (start_block or end_block):
        raise ValueError("Cannot use --days with --start-block/--end-block")

    if days:
        # T069: Calculate range from latest block
        response = requests.get(f"{ELECTRS_API_URL}/blocks/tip/height", timeout=10)
        response.raise_for_status()
        latest_height = int(response.text.strip())

        num_blocks = days * BLOCKS_PER_DAY
        start = latest_height - num_blocks
        end = latest_height

        logger.info(f"üìÖ Backtest range: {days} days ({num_blocks} blocks)")
        logger.info(f"   Start: block {start}")
        logger.info(f"   End: block {end}")

        return start, end

    elif start_block and end_block:
        # Custom range
        if end_block <= start_block:
            raise ValueError("end_block must be greater than start_block")

        num_blocks = end_block - start_block
        logger.info(f"üìÖ Custom block range: {num_blocks} blocks")
        logger.info(f"   Start: block {start_block}")
        logger.info(f"   End: block {end_block}")

        return start_block, end_block

    else:
        raise ValueError("Must provide either --days or --start-block + --end-block")


async def analyze_block_range(
    whale_detector: WhaleFlowDetector, start_block: int, end_block: int
) -> List[WhaleFlowSignal]:
    """
    T070: Analyze whale flow for a range of blocks (ASYNC with aiohttp session reuse).

    Args:
        whale_detector: Initialized WhaleFlowDetector instance
        start_block: Start block height
        end_block: End block height (exclusive)

    Returns:
        List of WhaleFlowSignal objects

    Note:
        PERFORMANCE OPTIMIZED: Reuses single aiohttp session across all blocks.
        This avoids recreating HTTP connections for every block (~360x speedup).
        Estimated time: ~3-5 seconds per block (was ~110 seconds with session recreation).
    """
    import aiohttp  # Local import to avoid linter issues

    signals = []
    total_blocks = end_block - start_block

    logger.info("üîç Starting backtest analysis...")
    logger.info(f"   Total blocks: {total_blocks}")
    logger.info(f"   Estimated time: {total_blocks * 5 / 60:.1f} hours")
    logger.info("   Progress logged every 10 blocks")

    # Create single session for all blocks (PERFORMANCE OPTIMIZATION)
    async with aiohttp.ClientSession() as session:
        for i, height in enumerate(range(start_block, end_block), start=1):
            try:
                # Use optimized method that reuses session
                signal = await whale_detector._analyze_block_with_session(
                    session, height
                )
                signals.append(signal)

                # Log progress every 10 blocks
                if i % 10 == 0 or i == 1:
                    pct = (i / total_blocks) * 100
                    elapsed = i * 5 / 60  # Rough estimate: 5 sec/block
                    remaining = (total_blocks - i) * 5 / 60
                    logger.info(
                        f"   [{i}/{total_blocks}] ({pct:.1f}%) - "
                        f"Block {height}: {signal.direction} "
                        f"({signal.net_flow_btc:+.1f} BTC) - "
                        f"Elapsed: {elapsed:.1f}h, Remaining: {remaining:.1f}h"
                    )

            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è  Block {height} failed: {e}. Skipping...")
                continue

    logger.info(f"‚úÖ Analysis complete: {len(signals)}/{total_blocks} blocks processed")
    return signals


def calculate_correlation(
    signals: List[WhaleFlowSignal], price_changes: List[float]
) -> float:
    """
    T074: Calculate Pearson correlation between whale net flow and 24h price change.

    Args:
        signals: List of WhaleFlowSignal objects
        price_changes: List of 24h price changes (in %, aligned with signals)

    Returns:
        Pearson correlation coefficient (-1.0 to 1.0)

    Note:
        Uses numpy if available, otherwise falls back to manual calculation.
    """
    if len(signals) != len(price_changes):
        raise ValueError(
            f"Signal count ({len(signals)}) != price change count ({len(price_changes)})"
        )

    if len(signals) < 2:
        logger.warning("Not enough data points for correlation (need at least 2)")
        return 0.0

    # Extract net flows
    net_flows = [s.net_flow_btc for s in signals]

    # Try numpy for efficiency
    try:
        import numpy as np

        corr = np.corrcoef(net_flows, price_changes)[0, 1]
        return float(corr) if not np.isnan(corr) else 0.0
    except ImportError:
        # Manual Pearson correlation
        n = len(net_flows)
        mean_flow = sum(net_flows) / n
        mean_price = sum(price_changes) / n

        numerator = sum(
            (net_flows[i] - mean_flow) * (price_changes[i] - mean_price)
            for i in range(n)
        )
        denom_flow = sum((x - mean_flow) ** 2 for x in net_flows) ** 0.5
        denom_price = sum((x - mean_price) ** 2 for x in price_changes) ** 0.5

        if denom_flow == 0 or denom_price == 0:
            return 0.0

        return numerator / (denom_flow * denom_price)


def calculate_false_positive_rate(
    signals: List[WhaleFlowSignal], price_changes: List[float]
) -> float:
    """
    T075: Calculate false positive rate for whale signals.

    Definition:
    - False positive: Signal predicts wrong direction
    - ACCUMULATION (bullish) but price goes down (negative change)
    - DISTRIBUTION (bearish) but price goes up (positive change)
    - NEUTRAL signals are excluded from calculation

    Args:
        signals: List of WhaleFlowSignal objects
        price_changes: List of 24h price changes (in %, aligned with signals)

    Returns:
        False positive rate (0.0 to 1.0)
    """
    if len(signals) != len(price_changes):
        raise ValueError(
            f"Signal count ({len(signals)}) != price change count ({len(price_changes)})"
        )

    false_positives = 0
    total_signals = 0

    for signal, price_change in zip(signals, price_changes):
        # Skip NEUTRAL signals (not actionable)
        if signal.direction == "NEUTRAL":
            continue

        total_signals += 1

        # Check if prediction was wrong
        if signal.direction == "ACCUMULATION" and price_change < 0:
            false_positives += 1  # Predicted bullish but price dropped
        elif signal.direction == "DISTRIBUTION" and price_change > 0:
            false_positives += 1  # Predicted bearish but price rose

    if total_signals == 0:
        logger.warning("No actionable signals (all NEUTRAL)")
        return 0.0

    return false_positives / total_signals


def save_backtest_results_to_db(
    signals: List[WhaleFlowSignal],
    price_data: List[Tuple[int, float]],
    db_path: str,
) -> None:
    """
    T071: Save backtest results to DuckDB.

    Creates table `backtest_whale_signals` with schema:
    - block_height: INT
    - net_flow_btc: FLOAT
    - direction: VARCHAR
    - confidence: FLOAT
    - btc_price: FLOAT (at block time)
    - timestamp: TIMESTAMP

    Args:
        signals: List of WhaleFlowSignal objects
        price_data: List of (block_height, btc_price) tuples
        db_path: Path to DuckDB database
    """
    import duckdb

    # Create price lookup dict
    price_dict = dict(price_data)

    conn = duckdb.connect(db_path)

    # Create table if not exists
    conn.execute("""
        CREATE TABLE IF NOT EXISTS backtest_whale_signals (
            block_height INTEGER PRIMARY KEY,
            net_flow_btc FLOAT,
            direction VARCHAR,
            confidence FLOAT,
            btc_price FLOAT,
            timestamp BIGINT,
            inflow_btc FLOAT,
            outflow_btc FLOAT,
            internal_btc FLOAT,
            tx_count_total INTEGER,
            tx_count_relevant INTEGER
        )
    """)

    # Insert signals
    for signal in signals:
        btc_price = price_dict.get(signal.block_height, None)

        conn.execute(
            """
            INSERT OR REPLACE INTO backtest_whale_signals VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """,
            [
                signal.block_height,
                signal.net_flow_btc,
                signal.direction,
                signal.confidence,
                btc_price,
                signal.timestamp,
                signal.inflow_btc,
                signal.outflow_btc,
                signal.internal_btc,
                signal.tx_count_total,
                signal.tx_count_relevant,
            ],
        )

    conn.commit()
    conn.close()

    logger.info(f"üíæ Saved {len(signals)} backtest signals to DuckDB: {db_path}")


def fetch_btc_prices_for_blocks(
    block_heights: List[int], db_path: str
) -> List[Tuple[int, float]]:
    """
    T072: Fetch BTC/USD prices for given block heights from DuckDB.

    Fetches from `price_analysis` table (mempool.space exchange prices).
    Uses block timestamp to match dates.

    Args:
        block_heights: List of block heights to fetch prices for
        db_path: Path to DuckDB database

    Returns:
        List of (block_height, btc_price) tuples

    Note:
        - Matches blocks to dates via timestamp
        - Returns None for prices if date not in database
        - Uses exchange_price column from price_analysis table
    """
    import duckdb
    import requests

    conn = duckdb.connect(db_path, read_only=True)

    # Check if price_analysis table exists and has data
    try:
        result = conn.execute("SELECT COUNT(*) FROM price_analysis").fetchone()
        if result[0] == 0:
            logger.warning(
                "price_analysis table is empty - will fetch from mempool.space API"
            )
            conn.close()
            return fetch_btc_prices_from_api(block_heights)
    except Exception as e:
        logger.warning(f"Cannot access price_analysis table: {e}")
        conn.close()
        return fetch_btc_prices_from_api(block_heights)

    # Fetch block timestamps from Bitcoin Core (via electrs)
    price_data = []

    for block_height in block_heights:
        try:
            # Get block hash from electrs
            response = requests.get(
                f"http://localhost:3001/block-height/{block_height}", timeout=10
            )
            block_hash = response.text.strip()

            # Get block header to extract timestamp
            response = requests.get(
                f"http://localhost:3001/block/{block_hash}", timeout=10
            )
            block_data = response.json()
            block_timestamp = block_data["timestamp"]

            # Convert timestamp to date
            from datetime import datetime

            block_date = datetime.fromtimestamp(block_timestamp).strftime("%Y-%m-%d")

            # Query DuckDB for price on that date
            result = conn.execute(
                "SELECT exchange_price FROM price_analysis WHERE date = ?",
                [block_date],
            ).fetchone()

            if result and result[0]:
                price_data.append((block_height, float(result[0])))
            else:
                logger.debug(
                    f"No price data for block {block_height} (date: {block_date})"
                )
                price_data.append((block_height, None))

        except Exception as e:
            logger.warning(f"Failed to fetch price for block {block_height}: {e}")
            price_data.append((block_height, None))

    conn.close()
    return price_data


def fetch_btc_prices_from_api(block_heights: List[int]) -> List[Tuple[int, float]]:
    """
    T072: Fallback - Fetch BTC/USD prices from mempool.space API.

    Args:
        block_heights: List of block heights

    Returns:
        List of (block_height, btc_price) tuples
    """
    import requests

    logger.info("üì° Fetching prices from mempool.space API (fallback)")

    price_data = []

    for block_height in block_heights:
        try:
            # Get block hash
            response = requests.get(
                f"http://localhost:3001/block-height/{block_height}", timeout=10
            )
            block_hash = response.text.strip()

            # Get block header
            response = requests.get(
                f"http://localhost:3001/block/{block_hash}", timeout=10
            )
            block_data = response.json()
            block_timestamp = block_data["timestamp"]

            # Fetch price from mempool.space public API
            response = requests.get(
                f"https://mempool.space/api/v1/historical-price?currency=USD&timestamp={block_timestamp}",
                timeout=10,
            )
            price_data_json = response.json()
            btc_price = price_data_json.get("prices", [{}])[0].get("USD", None)

            if btc_price:
                price_data.append((block_height, float(btc_price)))
            else:
                price_data.append((block_height, None))

        except Exception as e:
            logger.warning(f"Failed to fetch price for block {block_height}: {e}")
            price_data.append((block_height, None))

    return price_data


def calculate_price_changes_24h(
    price_data: List[Tuple[int, float]], lag_blocks: int = 144
) -> List[float]:
    """
    T073: Calculate 24h price changes (percentage) with lag.

    Args:
        price_data: List of (block_height, btc_price) tuples (sorted by height)
        lag_blocks: Number of blocks for lag (default: 144 = ~24 hours)

    Returns:
        List of percentage price changes (aligned with price_data)

    Note:
        - Returns 0.0 for first `lag_blocks` entries (no previous data)
        - Formula: ((price_future - price_now) / price_now) * 100
    """
    price_changes = []

    for i in range(len(price_data)):
        current_height, current_price = price_data[i]

        if current_price is None:
            price_changes.append(0.0)
            continue

        # Look ahead `lag_blocks` to get future price
        future_index = i + lag_blocks

        if future_index >= len(price_data):
            # No future data available
            price_changes.append(0.0)
            continue

        future_height, future_price = price_data[future_index]

        if future_price is None:
            price_changes.append(0.0)
            continue

        # Calculate percentage change
        price_change_pct = ((future_price - current_price) / current_price) * 100
        price_changes.append(price_change_pct)

    return price_changes


def generate_backtest_report(
    signals: List[WhaleFlowSignal],
    price_data: List[Tuple[int, float]],
    price_changes: List[float],
    correlation: float,
    false_positive_rate: float,
    start_block: int,
    end_block: int,
    report_path: str,
) -> None:
    """
    T076: Generate backtest validation report in markdown format.

    Creates a comprehensive report with:
    - Backtest summary (block range, duration)
    - Validation metrics (correlation, false positive rate)
    - Success criteria evaluation
    - Signal distribution statistics
    - Recommendations

    Args:
        signals: List of WhaleFlowSignal objects
        price_data: List of (block_height, btc_price) tuples
        price_changes: List of percentage price changes
        correlation: Correlation coefficient
        false_positive_rate: False positive rate (0.0 to 1.0)
        start_block: Start block height
        end_block: End block height
        report_path: Output path for markdown report
    """
    from datetime import datetime

    # Calculate statistics
    num_accumulation = sum(1 for s in signals if s.direction == "ACCUMULATION")
    num_distribution = sum(1 for s in signals if s.direction == "DISTRIBUTION")
    num_neutral = sum(1 for s in signals if s.direction == "NEUTRAL")
    avg_net_flow = (
        sum(s.net_flow_btc for s in signals) / len(signals) if signals else 0.0
    )

    # Success criteria evaluation
    sc002_pass = abs(correlation) > 0.6
    sc003_pass = false_positive_rate < 0.2

    # Generate report
    report = f"""# Whale Flow Backtest Validation Report

**Generated**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**Spec**: 004-whale-flow-detection
**Tasks**: T066-T076

---

## Backtest Summary

- **Block Range**: {start_block:,} ‚Üí {end_block:,} ({end_block - start_block:,} blocks)
- **Duration**: {(end_block - start_block) / 144:.1f} days (~{(end_block - start_block) / 144 * 24:.0f} hours)
- **Blocks Analyzed**: {len(signals):,}
- **Valid Data Points**: {sum(1 for _, p in price_data if p is not None):,} (with price data)

---

## Validation Metrics

### Correlation Analysis

- **Whale Net Flow vs Price Change (24h)**: `{correlation:.3f}`
- **Interpretation**: {"Strong negative correlation (expected)" if correlation < -0.6 else "Weak or no correlation"}
- **Note**: Negative correlation is correct (negative net_flow = outflow = bullish ‚Üí price rises)

### False Positive Rate

- **Rate**: `{false_positive_rate * 100:.1f}%`
- **Definition**: Percentage of signals predicting wrong direction
- **Calculation**:
  - ACCUMULATION but price drops = false positive
  - DISTRIBUTION but price rises = false positive
  - NEUTRAL signals excluded

---

## Success Criteria Evaluation

| Criterion | Target | Result | Status |
|-----------|--------|--------|--------|
| **SC-002**: Correlation | > 0.6 (absolute) | {abs(correlation):.3f} | {"‚úÖ PASS" if sc002_pass else "‚ùå FAIL"} |
| **SC-003**: False Positive Rate | < 20% | {false_positive_rate * 100:.1f}% | {"‚úÖ PASS" if sc003_pass else "‚ùå FAIL"} |

**Overall**: {"‚úÖ **BACKTEST PASSED**" if sc002_pass and sc003_pass else "‚ùå **BACKTEST FAILED**"}

---

## Signal Distribution

| Direction | Count | Percentage |
|-----------|-------|------------|
| ACCUMULATION (bullish) | {num_accumulation:,} | {num_accumulation / len(signals) * 100:.1f}% |
| DISTRIBUTION (bearish) | {num_distribution:,} | {num_distribution / len(signals) * 100:.1f}% |
| NEUTRAL | {num_neutral:,} | {num_neutral / len(signals) * 100:.1f}% |
| **Total** | **{len(signals):,}** | **100%** |

**Average Net Flow**: {avg_net_flow:+.2f} BTC

---

## Recommendations

"""

    # Add recommendations based on results
    if sc002_pass and sc003_pass:
        report += """### ‚úÖ Backtest Successful

The whale flow detector shows strong predictive power:
- High correlation with price movements
- Low false positive rate (<20%)
- Signal quality meets production criteria

**Next Steps**:
1. Deploy to production with confidence
2. Monitor live performance vs backtest results
3. Consider A/B testing against other indicators
4. Set up alerts for strong ACCUMULATION/DISTRIBUTION signals
"""
    else:
        report += """### ‚ö†Ô∏è Backtest Issues Detected

"""
        if not sc002_pass:
            report += f"""**Low Correlation ({abs(correlation):.3f})**:
- Whale flow signals may not be predictive of price movements
- Consider adjusting threshold (currently 100 BTC)
- Investigate if exchange address list is comprehensive
- Check for data quality issues (timestamps, price matching)

"""
        if not sc003_pass:
            report += f"""**High False Positive Rate ({false_positive_rate * 100:.1f}%)**:
- Signals frequently predict wrong direction
- May need to refine classification logic
- Consider adding confidence threshold filtering
- Investigate if thresholds need tuning

"""

        report += """**Recommended Actions**:
1. Review signal classification logic
2. Validate exchange address list completeness
3. Consider longer backtest period (7+ days recommended)
4. Analyze false positives manually to find patterns
5. DO NOT deploy to production until issues resolved
"""

    report += """
---

## Technical Details

### Implementation

- **Whale Detector**: `scripts/whale_flow_detector.py`
- **Backtest Script**: `scripts/whale_flow_backtest.py`
- **Exchange Addresses**: `data/exchange_addresses.csv`
- **Database**: `backtest_whale_signals` table in DuckDB

### Data Quality

- **Price Data Source**: DuckDB `price_analysis` table (mempool.space exchange prices)
- **Fallback**: mempool.space public API for missing dates
- **Block Timestamp Matching**: Via electrs HTTP API
- **24h Price Change**: Calculated with 144 block lag

### References

- **Spec**: `specs/004-whale-flow-detection/spec.md`
- **Tasks**: `specs/004-whale-flow-detection/tasks.md`
- **Contract**: `specs/004-whale-flow-detection/contracts/whale_flow_detector_interface.py`

---

**Report End**
"""

    # Write report
    with open(report_path, "w") as f:
        f.write(report)

    logger.info(f"üìÑ Validation report generated: {report_path}")


def main():
    """
    T066-T067: Main CLI for whale flow backtest.
    """
    parser = argparse.ArgumentParser(
        description="Whale Flow Backtest - Validate correlation with price movements"
    )
    parser.add_argument(
        "--days",
        type=int,
        help="Number of days to backtest (from latest block)",
    )
    parser.add_argument(
        "--start-block",
        type=int,
        help="Custom start block height",
    )
    parser.add_argument(
        "--end-block",
        type=int,
        help="Custom end block height",
    )
    parser.add_argument(
        "--csv",
        type=str,
        default="/media/sam/1TB/UTXOracle/data/exchange_addresses.csv",
        help="Path to exchange addresses CSV",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="backtest_results.json",
        help="Output file for results (JSON format)",
    )
    parser.add_argument(
        "--db-path",
        type=str,
        default="/media/sam/2TB-NVMe/prod/apps/utxoracle/data/utxoracle_cache.db",
        help="Path to DuckDB database",
    )
    parser.add_argument(
        "--report",
        type=str,
        default="docs/WHALE_FLOW_BACKTEST_REPORT.md",
        help="Output path for validation report (markdown)",
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose logging",
    )

    args = parser.parse_args()

    # Setup logging
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )

    # Validate arguments
    if not args.days and not (args.start_block and args.end_block):
        parser.error("Must provide either --days or --start-block + --end-block")

    if args.days and (args.start_block or args.end_block):
        parser.error("Cannot use --days with --start-block/--end-block")

    logger.info("üêã Whale Flow Backtest Starting...")
    logger.info(f"   Exchange addresses: {args.csv}")

    # Initialize whale detector
    whale_detector = WhaleFlowDetector(args.csv)
    logger.info(
        f"   ‚úÖ Whale detector initialized ({whale_detector.get_exchange_address_count()} addresses)"
    )

    # Calculate block range
    try:
        start_block, end_block = calculate_block_range(
            days=args.days, start_block=args.start_block, end_block=args.end_block
        )
    except Exception as e:
        logger.error(f"‚ùå Failed to calculate block range: {e}")
        sys.exit(1)

    # Analyze blocks (async with aiohttp)
    logger.info("")
    import asyncio

    signals = asyncio.run(analyze_block_range(whale_detector, start_block, end_block))

    if len(signals) == 0:
        logger.error("‚ùå No blocks analyzed successfully")
        sys.exit(1)

    # T072: Fetch BTC/USD price data
    logger.info("")
    logger.info("üí∞ Fetching BTC/USD price data...")
    block_heights = [s.block_height for s in signals]
    price_data = fetch_btc_prices_for_blocks(block_heights, args.db_path)

    # T073: Calculate 24h price changes
    logger.info("üìà Calculating 24h price changes...")
    price_changes = calculate_price_changes_24h(price_data, lag_blocks=144)

    # Filter out signals with no price data
    valid_signals = []
    valid_price_changes = []
    for i, (signal, price_change) in enumerate(zip(signals, price_changes)):
        if price_data[i][1] is not None and price_change != 0.0:
            valid_signals.append(signal)
            valid_price_changes.append(price_change)

    logger.info(
        f"   Valid data points: {len(valid_signals)}/{len(signals)} (with price data)"
    )

    # Calculate correlation and false positive rate
    if len(valid_signals) >= 2:
        correlation = calculate_correlation(valid_signals, valid_price_changes)
        false_positive_rate = calculate_false_positive_rate(
            valid_signals, valid_price_changes
        )
    else:
        logger.warning("Not enough valid data points for correlation analysis")
        correlation = 0.0
        false_positive_rate = 0.0

    # T071: Save to DuckDB
    logger.info("")
    logger.info("üíæ Saving backtest results to DuckDB...")
    save_backtest_results_to_db(signals, price_data, args.db_path)

    # Display results
    logger.info("")
    logger.info("üìä Backtest Results:")
    logger.info(f"   Blocks analyzed: {len(signals)}")
    logger.info(
        f"   ACCUMULATION signals: {sum(1 for s in signals if s.direction == 'ACCUMULATION')}"
    )
    logger.info(
        f"   DISTRIBUTION signals: {sum(1 for s in signals if s.direction == 'DISTRIBUTION')}"
    )
    logger.info(
        f"   NEUTRAL signals: {sum(1 for s in signals if s.direction == 'NEUTRAL')}"
    )
    logger.info(
        f"   Average net flow: {sum(s.net_flow_btc for s in signals) / len(signals):+.2f} BTC"
    )
    logger.info("")
    logger.info("üìà Validation Metrics:")
    logger.info(f"   Correlation (whale flow vs price): {correlation:.3f}")
    logger.info(f"   False positive rate: {false_positive_rate * 100:.1f}%")
    logger.info("")
    logger.info("üéØ Success Criteria:")
    logger.info(
        f"   SC-002: Correlation > 0.6 ‚Üí {'‚úÖ PASS' if abs(correlation) > 0.6 else '‚ùå FAIL'}"
    )
    logger.info(
        f"   SC-003: False positive < 20% ‚Üí {'‚úÖ PASS' if false_positive_rate < 0.2 else '‚ùå FAIL'}"
    )

    # T076: Generate validation report
    logger.info("")
    logger.info("üìÑ Generating validation report...")
    generate_backtest_report(
        signals=signals,
        price_data=price_data,
        price_changes=price_changes,
        correlation=correlation,
        false_positive_rate=false_positive_rate,
        start_block=start_block,
        end_block=end_block,
        report_path=args.report,
    )

    # Save results
    results = {
        "metadata": {
            "start_block": start_block,
            "end_block": end_block,
            "blocks_analyzed": len(signals),
            "timestamp": datetime.now().isoformat(),
        },
        "signals": [
            {
                "block_height": s.block_height,
                "net_flow_btc": s.net_flow_btc,
                "direction": s.direction,
                "confidence": s.confidence,
                "inflow_btc": s.inflow_btc,
                "outflow_btc": s.outflow_btc,
                "internal_btc": s.internal_btc,
                "tx_count_total": s.tx_count_total,
                "tx_count_relevant": s.tx_count_relevant,
            }
            for s in signals
        ],
    }

    with open(args.output, "w") as f:
        json.dump(results, f, indent=2)

    logger.info(f"   üíæ Results saved to: {args.output}")
    logger.info("")
    logger.info("‚úÖ Backtest complete!")
    logger.info("")
    logger.info("üìã Summary:")
    logger.info(f"   - Blocks analyzed: {len(signals):,}")
    logger.info(f"   - Correlation: {correlation:.3f}")
    logger.info(f"   - False positive rate: {false_positive_rate * 100:.1f}%")
    logger.info(f"   - Report: {args.report}")
    logger.info(f"   - Results: {args.output}")


if __name__ == "__main__":
    main()
